{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import ortho_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to generate random non singular covariance matrices with \n",
    "def generate_covariance_matrices(n_components, d):\n",
    "    ans = []\n",
    "    for i in range(n_components):\n",
    "        Q = ortho_group.rvs(d)\n",
    "        decay = 0.4*(1 + np.random.random())\n",
    "        eps = 1e-8\n",
    "        U = np.ones(d,) * (np.random.rand() * 20) + 600\n",
    "        for j in range(1,d):\n",
    "            U[j] = max((U[j-1] * decay) + (np.random.rand() * 0.01), eps)\n",
    "        ans.append(Q @ np.diag(U) @ Q.transpose())\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmm recovery from a synthetic gmm\n",
    "# make a representative gmm\n",
    "def make_gmm(n_components, n_features, random_state=0):\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "    gmm.means_ = np.random.rand(n_components, n_features) * 4\n",
    "    covariance_array = generate_covariance_matrices(n_components, n_features)\n",
    "    # for i in range(n_components):\n",
    "    #     rand_matrix = np.random.rand(n_features, n_features)\n",
    "    #     covariance_array.append(np.dot(rand_matrix, rand_matrix.T))\n",
    "\n",
    "    gmm.covariances_ = np.array(covariance_array)\n",
    "\n",
    "    precision_array = [np.linalg.pinv(cov) for cov in covariance_array]\n",
    "    gmm.precisions_ = np.array(precision_array)\n",
    "    gmm.precisions_cholesky_ = np.array([np.linalg.cholesky(prec) for prec in precision_array])\n",
    "    \n",
    "    gmm.weights_ = np.random.rand(n_components)\n",
    "    gmm.weights_ /= np.sum(gmm.weights_)\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ -5.26836486,  -7.88965868,   6.11643519, -11.45308188,\n",
       "           3.66742442,   7.74727604,  12.75144606,   2.19364309,\n",
       "          -8.06624039,   7.58596306,  -1.22982727, -25.68076378,\n",
       "         -10.86052499,  -3.29374238,   0.95878925,  18.40788379,\n",
       "         -35.95657899,  -8.66452726,   3.78172571,  -5.65498484]]),\n",
       " array([1]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 5\n",
    "n_features = 20\n",
    "\n",
    "my_test_gmm = make_gmm(n_components, n_features)\n",
    "my_test_gmm.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 14)\n",
      "(2000, 20)\n",
      "(14, 20)\n",
      "2000\n",
      "0.0018302139126426498\n"
     ]
    }
   ],
   "source": [
    "# take compressive meaurements\n",
    "# make 10 random measurement matrices\n",
    "n_measurements = ((n_features * 7) // 10) # 50% of the features\n",
    "\n",
    "measurement_matrices = []\n",
    "for i in range(10):\n",
    "    # measurement_matrices.append(np.random.randn(n_measurements, n_features))\n",
    "    measurement_matrices.append(np.random.binomial(1, 0.5, size=(n_measurements, n_features)))\n",
    "\n",
    "# generate samples from the gmm\n",
    "n_samples = 2000\n",
    "samples = my_test_gmm.sample(n_samples)\n",
    "\n",
    "# take measurements\n",
    "compressed_measurements = []\n",
    "sample_measurement_matrix = []\n",
    "for i in range(n_samples):\n",
    "    matrix_index = np.random.randint(0, len(measurement_matrices))\n",
    "    sample_measurement_matrix.append(matrix_index)\n",
    "    compressed_measurements.append(np.dot(measurement_matrices[matrix_index], samples[0][i]))\n",
    "\n",
    "\n",
    "noise_std_dev = 0.0005\n",
    "noise_covariance_matrix = np.eye(n_measurements) * (noise_std_dev * noise_std_dev)\n",
    "compressed_measurements = np.array(compressed_measurements)\n",
    "comp_copy = compressed_measurements.copy()\n",
    "compressed_measurements += np.random.normal(0, noise_std_dev, compressed_measurements.shape)\n",
    "print(compressed_measurements.shape)\n",
    "print(samples[0].shape)\n",
    "print(measurement_matrices[0].shape)\n",
    "print(len(sample_measurement_matrix))\n",
    "\n",
    "diff = 0\n",
    "for i in range(compressed_measurements.shape[0]):\n",
    "    diff += np.linalg.norm(compressed_measurements[i] - comp_copy[i])\n",
    "\n",
    "diff /= compressed_measurements.shape[0]\n",
    "print(diff)\n",
    "\n",
    "# my_test_gmm = GaussianMixture(n_components=n_components,  n_init=5, verbose=1, max_iter=200, init_params='random')\n",
    "# my_test_gmm.fit(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of the 10 measurement matrices we have a separate gmm in the y-domain so let us make all of them p(y|z)\n",
    "\n",
    "def form_y_gmms(x_gmm, measurement_matrices, noise_covariance_matrix):\n",
    "    gmm_list = []\n",
    "    noise_inverse = np.linalg.inv(noise_covariance_matrix)\n",
    "    for i in range(len(measurement_matrices)):\n",
    "        gmm_sample = GaussianMixture(n_components=n_components, random_state=0)\n",
    "        gmm_sample.means_ = x_gmm.means_ @ measurement_matrices[i].T\n",
    "        covariance_array = []\n",
    "\n",
    "        for j in range(n_components):\n",
    "            # C_ij = np.linalg.pinv(measurement_matrices[i].T @ noise_inverse @ measurement_matrices[i] + x_gmm.precisions_[j])\n",
    "            # temp_matrix = noise_inverse @ measurement_matrices[i] @ C_ij @ measurement_matrices[i].T @ noise_inverse\n",
    "            # covariance_array.append(np.linalg.pinv(noise_inverse - temp_matrix))\n",
    "            covariance_array.append(noise_covariance_matrix  + measurement_matrices[i] @ x_gmm.covariances_[j] @ measurement_matrices[i].T)\n",
    "\n",
    "        gmm_sample.covariances_ = np.array(covariance_array)\n",
    "        gmm_sample.weights_ = x_gmm.weights_\n",
    "\n",
    "        precision_array = [np.linalg.pinv(covariance_array[i]) for i in range(n_components)]\n",
    "        gmm_sample.precisions_ = np.array(precision_array)\n",
    "        gmm_sample.precisions_cholesky_ = np.array([np.linalg.cholesky(precision_array[i]) for i in range(n_components)])\n",
    "        \n",
    "        gmm_list.append(gmm_sample)\n",
    "    return gmm_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, gmm_list):\n",
    "    log_likelihood = 0\n",
    "    for i in range(compressed_measurements.shape[0]):\n",
    "        # print(gmm_list[sample_measurement_matrix[i]].score(compressed_measurements[i].reshape(1, -1)))\n",
    "        log_likelihood += gmm_list[sample_measurement_matrix[i]].score(compressed_measurements[i].reshape(1, -1))\n",
    "\n",
    "    return log_likelihood/len(compressed_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_weights(compressed_measurements, y_gmms, sample_measurement_matrix):\n",
    "    # calculate p_ik\n",
    "    new_weights = []\n",
    "    for k in range(n_components):\n",
    "        ssum = 0\n",
    "        for i in range(n_samples):\n",
    "            p_ik = y_gmms[sample_measurement_matrix[i]].weights_[k] * multivariate_normal.pdf(compressed_measurements[i], mean=y_gmms[sample_measurement_matrix[i]].means_[k], cov=y_gmms[sample_measurement_matrix[i]].covariances_[k]) / np.exp(y_gmms[sample_measurement_matrix[i]].score(compressed_measurements[i].reshape(1, -1)))\n",
    "            ssum += p_ik\n",
    "        new_weights.append(ssum)\n",
    "    new_weights = np.array(new_weights)\n",
    "    new_weights /= np.sum(new_weights)\n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_reconstruction(measurement_matrix, noise_covariance_matrix, x_covariance_matrix, x_mean, compressed_measurement):\n",
    "    \"\"\"\n",
    "    gets the x reconstruction for a compressed sample for the z^th component in x (specified by x_covariance_matrix and x_mean)\n",
    "    \"\"\"\n",
    "    # noise_inverse = np.linalg.pinv(noise_covariance_matrix)\n",
    "    # C_z = np.linalg.pinv(measurement_matrix.T @ noise_inverse @ measurement_matrix + x_precision_matrix)\n",
    "    C_z = np.linalg.inv(noise_covariance_matrix + (measurement_matrix @ x_covariance_matrix @ measurement_matrix.T))\n",
    "    x_reconstruction = x_mean + (x_covariance_matrix @ measurement_matrix.T @ C_z @ (compressed_measurement - measurement_matrix @ x_mean))\n",
    "    return x_reconstruction    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_means(compressed_measurements, y_gmms, sample_measurement_matrix, estimated_gmm, measurement_matrices, noise_covariance_matrix):\n",
    "    new_means = []\n",
    "    for k in range(n_components):\n",
    "        ssum = 0\n",
    "        psum = 0\n",
    "        for i in range(n_samples):\n",
    "            p_ik = y_gmms[sample_measurement_matrix[i]].weights_[k] * multivariate_normal.pdf(compressed_measurements[i], mean=y_gmms[sample_measurement_matrix[i]].means_[k], cov=y_gmms[sample_measurement_matrix[i]].covariances_[k]) / np.exp(y_gmms[sample_measurement_matrix[i]].score(compressed_measurements[i].reshape(1, -1)))\n",
    "            psum += p_ik\n",
    "            n_ik = get_x_reconstruction(measurement_matrices[sample_measurement_matrix[i]], noise_covariance_matrix, estimated_gmm.covariances_[k], estimated_gmm.means_[k], compressed_measurements[i])\n",
    "            ssum += p_ik * n_ik\n",
    "        new_means.append(ssum/psum)\n",
    "    new_means = np.array(new_means)\n",
    "    return new_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_covariances(compressed_measurements, y_gmms, sample_measurement_matrix, estimated_gmm, measurement_matrices, noise_covariance_matrix, new_means):\n",
    "    new_covariances = []\n",
    "    for k in range(n_components):\n",
    "        csum = 0\n",
    "        psum = 0\n",
    "        for i in range(n_samples):\n",
    "            measurement_matrix = measurement_matrices[sample_measurement_matrix[i]]\n",
    "            p_ik = y_gmms[sample_measurement_matrix[i]].weights_[k] * multivariate_normal.pdf(compressed_measurements[i], mean=y_gmms[sample_measurement_matrix[i]].means_[k], cov=y_gmms[sample_measurement_matrix[i]].covariances_[k]) / np.exp(y_gmms[sample_measurement_matrix[i]].score(compressed_measurements[i].reshape(1, -1)))\n",
    "            psum += p_ik\n",
    "            n_ik = get_x_reconstruction(measurement_matrices[sample_measurement_matrix[i]], noise_covariance_matrix, estimated_gmm.covariances_[k], estimated_gmm.means_[k], compressed_measurements[i])\n",
    "            C_ik = estimated_gmm.covariances_[k] - (estimated_gmm.covariances_[k] @ measurement_matrix.T @ np.linalg.inv(noise_covariance_matrix + (measurement_matrix @ estimated_gmm.covariances_[k] @ measurement_matrix.T)) @ measurement_matrix @ estimated_gmm.covariances_[k])\n",
    "            csum += p_ik * (C_ik + (n_ik - new_means[k]).reshape(-1, 1) @ (n_ik - new_means[k]).reshape(1, -1))\n",
    "        new_covariances.append(csum/psum)\n",
    "    new_covariances = np.array(new_covariances)\n",
    "    return new_covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check if the reconstruction is good using the suggested method (compare Max-Max estimate and MSE optimal estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_l2(v, mat):\n",
    "    if np.linalg.matrix_rank(mat) < mat.shape[0]:\n",
    "        # handles the case where the matrix is not invertible\n",
    "        m = v.T @ np.linalg.pinv(mat) @ v\n",
    "    else:\n",
    "        m = v.T @ np.linalg.inv(mat) @ v\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(model, A, y, noise_covariance_matrix):\n",
    "    x_hat = np.empty(model.means_.shape)\n",
    "    cost = []\n",
    "    var_noise = noise_covariance_matrix\n",
    "\n",
    "    for j in range(model.means_.shape[0]):\n",
    "        var_j, mu_j = model.covariances_[j], model.means_[j]\n",
    "        x_hat_j = var_j @ A.T @ np.linalg.inv(A @ var_j @ A.T + var_noise) @ (y - A @ mu_j) + mu_j\n",
    "        # print(np.linalg.det(var_j))\n",
    "        # print(var_j)\n",
    "        cost_j = weighted_l2(y - A @ x_hat_j, var_noise) + weighted_l2(x_hat_j - mu_j, var_j) + np.log(np.linalg.det(var_j))\n",
    "        # print(np.linalg.det(var_j))\n",
    "        x_hat[j] = x_hat_j\n",
    "        cost.append(cost_j)\n",
    "\n",
    "    j = np.argmin(cost)\n",
    "    # print(j)\n",
    "    return x_hat[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min MSE RMSE: 2.20722856689254\n",
      "MM RMSE: 2.2057181361134495\n"
     ]
    }
   ],
   "source": [
    "# min MSE scheme\n",
    "reconstructed_samples = []\n",
    "y_gmms = form_y_gmms(my_test_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    y_gmm = y_gmms[sample_measurement_matrix[i]]\n",
    "    reconstructed_sample = np.zeros(my_test_gmm.means_[0].shape)\n",
    "    psum = 0\n",
    "    for j in range(n_components):\n",
    "        p_ik = y_gmm.weights_[j] * multivariate_normal.pdf(compressed_measurements[i], mean=y_gmm.means_[j], cov=y_gmm.covariances_[j]) / np.exp(y_gmm.score(compressed_measurements[i].reshape(1, -1)))\n",
    "        psum += p_ik\n",
    "        n_ik = get_x_reconstruction(measurement_matrices[sample_measurement_matrix[i]], noise_covariance_matrix, my_test_gmm.covariances_[j], my_test_gmm.means_[j], compressed_measurements[i])\n",
    "        reconstructed_sample += p_ik * n_ik\n",
    "    assert abs(psum - 1) < 1e-3\n",
    "    reconstructed_samples.append(reconstructed_sample)\n",
    "\n",
    "reconstructed_samples = np.array(reconstructed_samples)\n",
    "test_rmse = np.sqrt(np.mean(np.sum((reconstructed_samples - samples[0]) ** 2, axis=1)))\n",
    "print(f\"Min MSE RMSE: {test_rmse}\")\n",
    "\n",
    "# max-max scheme\n",
    "mm_recon = []\n",
    "for i in range(n_samples):\n",
    "    mm_recon.append(decode(my_test_gmm, measurement_matrices[sample_measurement_matrix[i]], compressed_measurements[i], noise_covariance_matrix))\n",
    "mm_recon = np.array(mm_recon)\n",
    "mm_rmse = np.sqrt(np.mean(np.sum((mm_recon - samples[0]) ** 2, axis=1)))\n",
    "print(f\"MM RMSE: {mm_rmse}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.21457588  2.92504666 -2.4157243  -0.53222906  8.68822933  2.14010323\n",
      " -3.92733048  7.54069754  0.26874633 -3.05725303  0.08985494 -1.7208007\n",
      "  7.53479582  2.38227858  3.81573647  2.32245132  0.46094808  4.74985201\n",
      "  7.6257281   2.7774674 ]\n",
      "[ 8.21457588  2.92504666 -2.4157243  -0.53222906  8.68822933  2.14010323\n",
      " -3.92733048  7.54069754  0.26874633 -3.05725303  0.08985494 -1.7208007\n",
      "  7.53479581  2.38227858  3.81573647  2.32245131  0.46094808  4.74985201\n",
      "  7.62572809  2.7774674 ]\n",
      "[ 8.11626355  3.0888958  -2.87547694  0.02393244  8.85621109  1.97785846\n",
      " -4.18169281  7.74912808 -0.19325706 -3.52072684 -0.02607054 -1.10046749\n",
      "  7.01940333  3.1064015   4.82483983  2.46493158  0.05849487  4.70260456\n",
      "  7.84598354  1.92990974]\n"
     ]
    }
   ],
   "source": [
    "print(mm_recon[0])\n",
    "print(reconstructed_samples[0])\n",
    "print(samples[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial log likelihood:  -39.036074155716136\n",
      "Ideal log likelihood:  -31.17322041000491\n",
      "Iteration 1 log-likelihood: -34.34883254506644\n",
      "Iteration 2 log-likelihood: -33.789852385753115\n",
      "Iteration 3 log-likelihood: -33.531102410495926\n",
      "Iteration 4 log-likelihood: -33.362681325089525\n",
      "Iteration 5 log-likelihood: -33.238355730546914\n",
      "Iteration 6 log-likelihood: -33.14057934403694\n",
      "Iteration 7 log-likelihood: -33.061087177593116\n",
      "Iteration 8 log-likelihood: -32.995169990813444\n",
      "Iteration 9 log-likelihood: -32.93964186727326\n",
      "Iteration 10 log-likelihood: -32.89237368062056\n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "# initialise estimate of the original gmm\n",
    "# assumption is that we know the number of components required to model the original gmm\n",
    "estimated_gmm = make_gmm(n_components, n_features, random_state=7)\n",
    "\n",
    "# perform updates on the estimated gmm\n",
    "# log likelihoods stores the log likelihoods of the estimated gmm computed on the samples drawn from the original gmm, our objective is to maximise this\n",
    "tol = 0.05\n",
    "log_likelihoods = []\n",
    "y_gmms = form_y_gmms(estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "log_likelihoods.append(get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, y_gmms))\n",
    "\n",
    "print(\"Initial log likelihood: \", log_likelihoods[-1])\n",
    "ideal_log_likelihood = get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, form_y_gmms(my_test_gmm, measurement_matrices, noise_covariance_matrix))\n",
    "print(\"Ideal log likelihood: \", ideal_log_likelihood)\n",
    "\n",
    "noise_inverse =  np.linalg.inv(noise_covariance_matrix)\n",
    "iter_counter = 0\n",
    "\n",
    "while True:\n",
    "    # perform updates\n",
    "    # for each of the 10 measurement matrices we have a separate gmm in the y-domain so let us make all of them\n",
    "    # update weights\n",
    "    # new_weights = []\n",
    "    # for i in range(n_components):\n",
    "    #     ssum = 0\n",
    "    #     for j in range(len(compressed_measurements)):\n",
    "    #         den = 0\n",
    "    #         for k in range(n_components):\n",
    "    #             den += estimated_gmm.weights_[k] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[k], cov=y_gmms[sample_measurement_matrix[j]].covariances_[k])\n",
    "    #         ssum += estimated_gmm.weights_[i] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[i], cov=y_gmms[sample_measurement_matrix[j]].covariances_[i]) / den\n",
    "    #     new_weights.append(ssum) # entry appended was \\sum_{i = 1}^{N} p_ik for k as the iter counter (here 'i')\n",
    "    # new_weights = np.array(new_weights)\n",
    "    # # new_weights /= np.sum(new_weights) # do this after mean and covariance update\n",
    "\n",
    "    # # update means\n",
    "    # new_means = []\n",
    "    # for i in range(n_components):\n",
    "    #     ssum = 0\n",
    "    #     psum = 0\n",
    "    #     for j in range(len(compressed_measurements)):\n",
    "    #         den = 0\n",
    "    #         for k in range(n_components):\n",
    "    #             den += estimated_gmm.weights_[k] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[k], cov=y_gmms[sample_measurement_matrix[j]].covariances_[k])\n",
    "    #         p_ik = estimated_gmm.weights_[i] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[i], cov=y_gmms[sample_measurement_matrix[j]].covariances_[i]) / den\n",
    "    #         C_ik = np.linalg.pinv(measurement_matrices[sample_measurement_matrix[j]].T @ noise_inverse @ measurement_matrices[sample_measurement_matrix[j]] + estimated_gmm.precisions_[i])\n",
    "    #         n_ik = estimated_gmm.means_[i] + (C_ik @ measurement_matrices[sample_measurement_matrix[j]].T @ noise_inverse @ (compressed_measurements[j] - measurement_matrices[sample_measurement_matrix[j]] @ estimated_gmm.means_[i]))\n",
    "    #         ssum += p_ik * n_ik\n",
    "    #         psum += p_ik\n",
    "    #     new_means.append(ssum / psum)\n",
    "\n",
    "    # # update covariances\n",
    "    # new_covariances = []\n",
    "    # for i in range(n_components):\n",
    "    #     ssum = 0\n",
    "    #     psum = 0\n",
    "    #     for j in range(len(compressed_measurements)):\n",
    "    #         den = 0\n",
    "    #         for k in range(n_components):\n",
    "    #             den += y_gmms[sample_measurement_matrix[j]].weights_[k] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[k], cov=y_gmms[sample_measurement_matrix[j]].covariances_[k])\n",
    "    #         p_ik = y_gmms[sample_measurement_matrix[j]].weights_[i] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[i], cov=y_gmms[sample_measurement_matrix[j]].covariances_[i]) / den\n",
    "    #         C_ik = np.linalg.pinv((measurement_matrices[sample_measurement_matrix[j]].T @ noise_inverse @ measurement_matrices[sample_measurement_matrix[j]]) + estimated_gmm.precisions_[i])\n",
    "    #         n_ik = estimated_gmm.means_[i] + C_ik @ (measurement_matrices[sample_measurement_matrix[j]].T @ noise_inverse @ (compressed_measurements[j] - measurement_matrices[sample_measurement_matrix[j]] @ estimated_gmm.means_[i]))\n",
    "    #         ssum += p_ik * ((n_ik - new_means[i]) @ (n_ik - new_means[i]).T + C_ik)\n",
    "    #         psum += p_ik\n",
    "    #     new_covariances.append(ssum / psum)\n",
    "\n",
    "\n",
    "    estimated_gmm.weights_ = get_new_weights(compressed_measurements, y_gmms, sample_measurement_matrix)\n",
    "    estimated_gmm.means_ = get_new_means(compressed_measurements, y_gmms, sample_measurement_matrix, estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "    estimated_gmm.covariances_ = get_new_covariances(compressed_measurements, y_gmms, sample_measurement_matrix, estimated_gmm, measurement_matrices, noise_covariance_matrix, estimated_gmm.means_)\n",
    "    precision_array = [np.linalg.pinv(cov) for cov in estimated_gmm.covariances_]\n",
    "    estimated_gmm.precisions_ = np.array(precision_array)\n",
    "    estimated_gmm.precisions_cholesky_ = np.array([np.linalg.cholesky(prec) for prec in estimated_gmm.precisions_])\n",
    "\n",
    "    y_gmms = form_y_gmms(estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "    log_likelihoods.append(get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, y_gmms))\n",
    "    print(f\"Iteration {iter_counter + 1} log-likelihood: {log_likelihoods[-1]}\")\n",
    "    iter_counter += 1\n",
    "    if abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol or log_likelihoods[-1] > ideal_log_likelihood:\n",
    "        if abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol:\n",
    "            print(\"Converged\")\n",
    "        else:\n",
    "            print(\"Ideal log-likelihood reached, stopping to avoid overfitting\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With initial conditions\n",
    "The following code is assuming only a slight deviation in the gmm, hence estimated gmm is initialised as being close to the original gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to shift gmm\n",
    "def shift_gmm(estimated_gmm, original_gmm, fc):\n",
    "    # shift means randomly by fc% of its norm and copy everything else as is\n",
    "    for i in range(n_components):\n",
    "        estimated_gmm.means_[i] = original_gmm.means_[i] + (fc * np.sqrt(np.linalg.norm(original_gmm.means_[i])**2 / n_features) * np.random.randn(estimated_gmm.means_[i].shape[0]))\n",
    "        estimated_gmm.covariances_[i] = original_gmm.covariances_[i]\n",
    "        estimated_gmm.precisions_[i] = original_gmm.precisions_[i]\n",
    "        estimated_gmm.precisions_cholesky_[i] = original_gmm.precisions_cholesky_[i]\n",
    "        estimated_gmm.weights_ = original_gmm.weights_\n",
    "\n",
    "    return estimated_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial log likelihood:  -46.28327186778595\n",
      "Ideal log likelihood:  -41.666608115490384\n",
      "Iteration 1 log-likelihood: -42.105632451875685\n",
      "Iteration 2 log-likelihood: -41.82562429325525\n",
      "Iteration 3 log-likelihood: -41.68822249015458\n",
      "Iteration 4 log-likelihood: -41.60120457405397\n",
      "Iteration 5 log-likelihood: -41.54266685957666\n",
      "Iteration 6 log-likelihood: -41.50169443941766\n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "# initialise estimate of the original gmm\n",
    "# assumption is that we know the number of components required to model the original gmm\n",
    "estimated_gmm = make_gmm(n_components, n_features, random_state=7)\n",
    "estimated_gmm = shift_gmm(estimated_gmm,my_test_gmm, 0.1)\n",
    "# for i in range(n_components):\n",
    "#     # shift mean randomly by 10% of its norm\n",
    "#     print(f\"Initial mean {i}: {estimated_gmm.means_[i]}\")\n",
    "#     print(f\"Original mean {i}: {my_test_gmm.means_[i]}\")\n",
    "\n",
    "# perform updates on the estimated gmm\n",
    "# log likelihoods stores the log likelihoods of the estimated gmm computed on the samples drawn from the original gmm, our objective is to maximise this\n",
    "tol = 0.05\n",
    "log_likelihoods = []\n",
    "y_gmms = form_y_gmms(estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "log_likelihoods.append(get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, y_gmms))\n",
    "\n",
    "print(\"Initial log likelihood: \", log_likelihoods[-1])\n",
    "ideal_log_likelihood = get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, form_y_gmms(my_test_gmm, measurement_matrices, noise_covariance_matrix))\n",
    "print(\"Ideal log likelihood: \", ideal_log_likelihood)\n",
    "\n",
    "noise_inverse =  np.linalg.inv(noise_covariance_matrix)\n",
    "iter_counter = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    new_weights = get_new_weights(compressed_measurements, y_gmms, sample_measurement_matrix)\n",
    "    new_means = get_new_means(compressed_measurements, y_gmms, sample_measurement_matrix, estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "    new_covariances = get_new_covariances(compressed_measurements, y_gmms, sample_measurement_matrix, estimated_gmm, measurement_matrices, noise_covariance_matrix, new_means)\n",
    "    # estimated_gmm.weights_ = get_new_weights(compressed_measurements, y_gmms, sample_measurement_matrix)\n",
    "    # estimated_gmm.means_ = get_new_means(compressed_measurements, y_gmms, sample_measurement_matrix, estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "    # estimated_gmm.covariances_ = get_new_covariances(compressed_measurements, y_gmms, sample_measurement_matrix, estimated_gmm, measurement_matrices, noise_covariance_matrix, estimated_gmm.means_)\n",
    "    estimated_gmm.weights_ = new_weights\n",
    "    estimated_gmm.means_ = new_means\n",
    "    estimated_gmm.covariances_ = new_covariances\n",
    "    \n",
    "    precision_array = [np.linalg.pinv(cov) for cov in estimated_gmm.covariances_]\n",
    "    estimated_gmm.precisions_ = np.array(precision_array)\n",
    "    estimated_gmm.precisions_cholesky_ = np.array([np.linalg.cholesky(prec) for prec in estimated_gmm.precisions_])\n",
    "\n",
    "    y_gmms = form_y_gmms(estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "    log_likelihoods.append(get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, y_gmms))\n",
    "    print(f\"Iteration {iter_counter + 1} log-likelihood: {log_likelihoods[-1]}\")\n",
    "    iter_counter += 1\n",
    "    if abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol or log_likelihoods[-1] > ideal_log_likelihood:\n",
    "        if abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol:\n",
    "            print(\"Converged\")\n",
    "            break\n",
    "        # else:\n",
    "        #     print(\"Ideal log-likelihood reached, stopping to avoid overfitting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.058868173032122356\n",
      "Test RMSE with original GMM: 0.05566008007988325\n"
     ]
    }
   ],
   "source": [
    "# get test sample set, take compressive measurements, reconstruct and check rmse\n",
    "n_test = 500\n",
    "test_sample_set = my_test_gmm.sample(n_samples=n_test)[0]\n",
    "test_compressed_measurements = []\n",
    "test_sample_matrix_indices = []\n",
    "for i in range(n_test):\n",
    "    matrix_index = np.random.randint(0, len(measurement_matrices))\n",
    "    test_sample_matrix_indices.append(matrix_index)\n",
    "    test_compressed_measurements.append(measurement_matrices[matrix_index] @ test_sample_set[i])\n",
    "\n",
    "test_reconstructed_samples = []\n",
    "test_reconstructed_samples_with_original_gmm = []\n",
    "y_gmms = form_y_gmms(estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "y_gmms_orig = form_y_gmms(my_test_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "\n",
    "for i in range(n_test):\n",
    "    y_gmm = y_gmms[test_sample_matrix_indices[i]]\n",
    "    y_gmm_orig = y_gmms_orig[test_sample_matrix_indices[i]]\n",
    "    reconstructed_sample = np.zeros(estimated_gmm.means_[0].shape)\n",
    "    reconstructed_sample_orig = np.zeros(my_test_gmm.means_[0].shape)\n",
    "    psum = 0\n",
    "    psum_orig = 0\n",
    "    for j in range(n_components):\n",
    "        p_ik = y_gmm.weights_[j] * multivariate_normal.pdf(test_compressed_measurements[i], mean=y_gmm.means_[j], cov=y_gmm.covariances_[j]) / np.exp(y_gmm.score(test_compressed_measurements[i].reshape(1, -1)))\n",
    "        p_ik_orig = y_gmm_orig.weights_[j] * multivariate_normal.pdf(test_compressed_measurements[i], mean=y_gmm_orig.means_[j], cov=y_gmm_orig.covariances_[j]) / np.exp(y_gmm_orig.score(test_compressed_measurements[i].reshape(1, -1)))\n",
    "        psum += p_ik\n",
    "        psum_orig += p_ik_orig\n",
    "        n_ik = get_x_reconstruction(measurement_matrices[test_sample_matrix_indices[i]], noise_covariance_matrix, estimated_gmm.covariances_[j], estimated_gmm.means_[j], test_compressed_measurements[i])\n",
    "        n_ik_orig = get_x_reconstruction(measurement_matrices[test_sample_matrix_indices[i]], noise_covariance_matrix, my_test_gmm.covariances_[j], my_test_gmm.means_[j], test_compressed_measurements[i])\n",
    "        reconstructed_sample += p_ik * n_ik\n",
    "        reconstructed_sample_orig += p_ik_orig * n_ik_orig\n",
    "    assert abs(psum - 1) < 1e-3\n",
    "    assert abs(psum_orig - 1) < 1e-3\n",
    "    test_reconstructed_samples.append(reconstructed_sample)\n",
    "    test_reconstructed_samples_with_original_gmm.append(reconstructed_sample_orig)\n",
    "\n",
    "test_reconstructed_samples = np.array(test_reconstructed_samples)\n",
    "test_reconstructed_samples_with_original_gmm = np.array(test_reconstructed_samples_with_original_gmm)\n",
    "test_rmse = 0\n",
    "for i in range(n_test):\n",
    "    test_rmse += np.linalg.norm(test_reconstructed_samples[i] - test_sample_set[i]) / np.linalg.norm(test_sample_set[i])\n",
    "test_rmse /= n_test\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "\n",
    "test_rmse_orig = 0\n",
    "for i in range(n_test):\n",
    "    test_rmse_orig += np.linalg.norm(test_reconstructed_samples_with_original_gmm[i] - test_sample_set[i]) / np.linalg.norm(test_sample_set[i])\n",
    "test_rmse_orig /= n_test\n",
    "print(f\"Test RMSE with original GMM: {test_rmse_orig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.60439257  -4.98422776  -8.10066273   3.24903175  11.54513042\n",
      "  -1.35877931  17.47838942   0.53470597   9.39891469  22.91456363\n",
      "   1.06972214 -18.96815579   3.26234575  19.3436277   -4.31814903\n",
      " -11.14217858  -4.13033329  -2.07150076   2.08938565   6.97327998]\n",
      "[  9.57991945  -5.24076535  -7.66024556   2.86350325  11.76178894\n",
      "  -1.36240888  17.89306714   0.62983187   9.40577758  22.52490605\n",
      "   0.9907406  -18.78789717   2.78753183  19.1747585   -4.4136155\n",
      " -10.68310346  -4.18305998  -1.95821821   2.39120026   7.22455477]\n"
     ]
    }
   ],
   "source": [
    "print(test_sample_set[0])\n",
    "print(test_reconstructed_samples[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
