{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmm recovery from a synthetic gmm\n",
    "# make a representative gmm\n",
    "def make_gmm(n_components, n_features, random_state=0):\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "    gmm.means_ = np.random.rand(n_components, n_features) * 4\n",
    "    covariance_array = []\n",
    "    for i in range(n_components):\n",
    "        rand_matrix = np.random.rand(n_features, n_features)\n",
    "        covariance_array.append(np.dot(rand_matrix, rand_matrix.T))\n",
    "\n",
    "    gmm.covariances_ = np.array(covariance_array)\n",
    "\n",
    "    precision_array = [np.linalg.pinv(cov) for cov in covariance_array]\n",
    "    gmm.precisions_ = np.array(precision_array)\n",
    "    gmm.precisions_cholesky_ = np.array([np.linalg.cholesky(prec) for prec in precision_array])\n",
    "    \n",
    "    gmm.weights_ = np.random.rand(n_components)\n",
    "    gmm.weights_ /= np.sum(gmm.weights_)\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-3.40444146, -0.24289984, -4.38096176, -1.94692586, -1.58085157,\n",
       "         -2.26139623, -1.01832417,  0.47809126, -3.63421823, -1.90995618,\n",
       "         -4.79551799, -3.16258518, -3.56713724, -2.23172194, -1.81130086,\n",
       "         -0.38178071,  0.14014666, -1.62091995, -0.753183  , -2.46930297]]),\n",
       " array([4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 5\n",
    "n_features = 20\n",
    "\n",
    "my_test_gmm = make_gmm(n_components, n_features)\n",
    "my_test_gmm.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n",
      "(5000, 20)\n",
      "(10, 20)\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "# take compressive meaurements\n",
    "# make 10 random measurement matrices\n",
    "n_measurements = ((n_features * 5) // 10) # 50% of the features\n",
    "\n",
    "measurement_matrices = []\n",
    "for i in range(10):\n",
    "    measurement_matrices.append(np.random.randn(n_measurements, n_features))\n",
    "\n",
    "# generate samples from the gmm\n",
    "n_samples = 5000\n",
    "samples = my_test_gmm.sample(n_samples)\n",
    "\n",
    "# take measurements\n",
    "compressed_measurements = []\n",
    "sample_measurement_matrix = []\n",
    "for i in range(n_samples):\n",
    "    matrix_index = np.random.randint(0, len(measurement_matrices))\n",
    "    sample_measurement_matrix.append(matrix_index)\n",
    "    compressed_measurements.append(np.dot(measurement_matrices[matrix_index], samples[0][i]))\n",
    "\n",
    "\n",
    "noise_std_dev = 0.05\n",
    "noise_covariance_matrix = np.eye(n_measurements) * (noise_std_dev * noise_std_dev)\n",
    "compressed_measurements = np.array(compressed_measurements)\n",
    "compressed_measurements += np.random.normal(0, noise_std_dev, compressed_measurements.shape)\n",
    "print(compressed_measurements.shape)\n",
    "print(samples[0].shape)\n",
    "print(measurement_matrices[0].shape)\n",
    "print(len(sample_measurement_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of the 10 measurement matrices we have a separate gmm in the y-domain so let us make all of them p(y|z)\n",
    "\n",
    "def form_y_gmms(x_gmm, measurement_matrices, noise_covariance_matrix):\n",
    "    gmm_list = []\n",
    "    noise_inverse = np.linalg.inv(noise_covariance_matrix)\n",
    "    for i in range(len(measurement_matrices)):\n",
    "        gmm_sample = GaussianMixture(n_components=n_components, random_state=0)\n",
    "        gmm_sample.means_ = x_gmm.means_ @ measurement_matrices[i].T\n",
    "        covariance_array = []\n",
    "\n",
    "        for j in range(n_components):\n",
    "            C_ij = np.linalg.pinv(measurement_matrices[i].T @ noise_inverse @ measurement_matrices[i] + x_gmm.precisions_[j])\n",
    "            temp_matrix = noise_inverse @ measurement_matrices[i] @ C_ij @ measurement_matrices[i].T @ noise_inverse\n",
    "            covariance_array.append(np.linalg.pinv(noise_inverse - temp_matrix))\n",
    "\n",
    "        gmm_sample.covariances_ = np.array(covariance_array)\n",
    "        gmm_sample.weights_ = x_gmm.weights_\n",
    "\n",
    "        precision_array = [np.linalg.pinv(covariance_array[i]) for i in range(n_components)]\n",
    "        gmm_sample.precisions_ = np.array(precision_array)\n",
    "        gmm_sample.precisions_cholesky_ = np.array([np.linalg.cholesky(precision_array[i]) for i in range(n_components)])\n",
    "        \n",
    "        gmm_list.append(gmm_sample)\n",
    "    return gmm_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, gmm_list):\n",
    "    log_likelihood = 0\n",
    "    for i in range(compressed_measurements.shape[0]):\n",
    "        # print(gmm_list[sample_measurement_matrix[i]].score(compressed_measurements[i].reshape(1, -1)))\n",
    "        log_likelihood += gmm_list[sample_measurement_matrix[i]].score(compressed_measurements[i].reshape(1, -1))\n",
    "\n",
    "    return log_likelihood/len(compressed_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_weights(compressed_measurements, y_gmms, sample_measurement_matrix):\n",
    "    # calculate p_ik\n",
    "    new_weights = []\n",
    "    for k in range(n_components):\n",
    "        ssum = 0\n",
    "        for i in range(n_samples):\n",
    "            p_ik = y_gmms[sample_measurement_matrix[i]].weights_[k] * multivariate_normal.pdf(compressed_measurements[i], mean=y_gmms[sample_measurement_matrix[i]].means_[k], cov=y_gmms[sample_measurement_matrix[i]].covariances_[k]) / np.exp(y_gmms[sample_measurement_matrix[i]].score(compressed_measurements[i].reshape(1, -1)))\n",
    "            ssum += p_ik\n",
    "        new_weights.append(ssum)\n",
    "    new_weights = np.array(new_weights)\n",
    "    new_weights /= np.sum(new_weights)\n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_reconstruction(measurement_matrix, noise_covariance_matrix, x_precision_matrix, x_mean, compressed_measurement):\n",
    "    \"\"\"\n",
    "    gets the x reconstruction for a compressed sample for the z^th component in x (specified by x_covariance_matrix and x_mean)\n",
    "    \"\"\"\n",
    "    noise_inverse = np.linalg.pinv(noise_covariance_matrix)\n",
    "    C_z = np.linalg.pinv(measurement_matrix.T @ noise_inverse @ measurement_matrix + x_precision_matrix)\n",
    "    x_reconstruction = x_mean + C_z @ measurement_matrix.T @ noise_inverse @ (compressed_measurement - measurement_matrix @ x_mean)\n",
    "    return x_reconstruction    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_means(compressed_measurements, y_gmms, sample_measurement_matrix, estimated_gmm, measurement_matrices, noise_covariance_matrix):\n",
    "    new_means = []\n",
    "    for k in range(n_components):\n",
    "        ssum = 0\n",
    "        psum = 0\n",
    "        for i in range(n_samples):\n",
    "            p_ik = y_gmms[sample_measurement_matrix[i]].weights_[k] * multivariate_normal.pdf(compressed_measurements[i], mean=y_gmms[sample_measurement_matrix[i]].means_[k], cov=y_gmms[sample_measurement_matrix[i]].covariances_[k]) / np.exp(y_gmms[sample_measurement_matrix[i]].score(compressed_measurements[i].reshape(1, -1)))\n",
    "            psum += p_ik\n",
    "            n_ik = get_x_reconstruction(measurement_matrices[sample_measurement_matrix[i]], noise_covariance_matrix, estimated_gmm.precisions_[k], estimated_gmm.means_[k], compressed_measurements[i])\n",
    "            ssum += p_ik * n_ik\n",
    "        new_means.append(ssum/psum)\n",
    "    new_means = np.array(new_means)\n",
    "    return new_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_covariances(compressed_measurements, y_gmms, sample_measurement_matrix, estimated_gmm, measurement_matrices, noise_covariance_matrix, new_means):\n",
    "    new_covariances = []\n",
    "    for k in range(n_components):\n",
    "        csum = 0\n",
    "        psum = 0\n",
    "        for i in range(n_samples):\n",
    "            p_ik = y_gmms[sample_measurement_matrix[i]].weights_[k] * multivariate_normal.pdf(compressed_measurements[i], mean=y_gmms[sample_measurement_matrix[i]].means_[k], cov=y_gmms[sample_measurement_matrix[i]].covariances_[k]) / np.exp(y_gmms[sample_measurement_matrix[i]].score(compressed_measurements[i].reshape(1, -1)))\n",
    "            psum += p_ik\n",
    "            n_ik = get_x_reconstruction(measurement_matrices[sample_measurement_matrix[i]], noise_covariance_matrix, estimated_gmm.precisions_[k], estimated_gmm.means_[k], compressed_measurements[i])\n",
    "            C_ik = np.linalg.pinv(measurement_matrices[sample_measurement_matrix[i]].T @ np.linalg.pinv(noise_covariance_matrix) @ measurement_matrices[sample_measurement_matrix[i]] + estimated_gmm.precisions_[k])\n",
    "            csum += p_ik * (C_ik + (n_ik - new_means[k]).reshape(-1, 1) @ (n_ik - new_means[k]).reshape(1, -1))\n",
    "        new_covariances.append(csum/psum)\n",
    "    new_covariances = np.array(new_covariances)\n",
    "    return new_covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial log likelihood:  -39.036074155716136\n",
      "Ideal log likelihood:  -31.17322041000491\n",
      "Iteration 1 log-likelihood: -34.34883254506644\n",
      "Iteration 2 log-likelihood: -33.789852385753115\n",
      "Iteration 3 log-likelihood: -33.531102410495926\n",
      "Iteration 4 log-likelihood: -33.362681325089525\n",
      "Iteration 5 log-likelihood: -33.238355730546914\n",
      "Iteration 6 log-likelihood: -33.14057934403694\n",
      "Iteration 7 log-likelihood: -33.061087177593116\n",
      "Iteration 8 log-likelihood: -32.995169990813444\n",
      "Iteration 9 log-likelihood: -32.93964186727326\n",
      "Iteration 10 log-likelihood: -32.89237368062056\n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "# initialise estimate of the original gmm\n",
    "# assumption is that we know the number of components required to model the original gmm\n",
    "estimated_gmm = make_gmm(n_components, n_features, random_state=7)\n",
    "\n",
    "# perform updates on the estimated gmm\n",
    "# log likelihoods stores the log likelihoods of the estimated gmm computed on the samples drawn from the original gmm, our objective is to maximise this\n",
    "tol = 0.05\n",
    "log_likelihoods = []\n",
    "y_gmms = form_y_gmms(estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "log_likelihoods.append(get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, y_gmms))\n",
    "\n",
    "print(\"Initial log likelihood: \", log_likelihoods[-1])\n",
    "ideal_log_likelihood = get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, form_y_gmms(my_test_gmm, measurement_matrices, noise_covariance_matrix))\n",
    "print(\"Ideal log likelihood: \", ideal_log_likelihood)\n",
    "\n",
    "noise_inverse =  np.linalg.inv(noise_covariance_matrix)\n",
    "iter_counter = 0\n",
    "\n",
    "while True:\n",
    "    # perform updates\n",
    "    # for each of the 10 measurement matrices we have a separate gmm in the y-domain so let us make all of them\n",
    "    # update weights\n",
    "    # new_weights = []\n",
    "    # for i in range(n_components):\n",
    "    #     ssum = 0\n",
    "    #     for j in range(len(compressed_measurements)):\n",
    "    #         den = 0\n",
    "    #         for k in range(n_components):\n",
    "    #             den += estimated_gmm.weights_[k] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[k], cov=y_gmms[sample_measurement_matrix[j]].covariances_[k])\n",
    "    #         ssum += estimated_gmm.weights_[i] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[i], cov=y_gmms[sample_measurement_matrix[j]].covariances_[i]) / den\n",
    "    #     new_weights.append(ssum) # entry appended was \\sum_{i = 1}^{N} p_ik for k as the iter counter (here 'i')\n",
    "    # new_weights = np.array(new_weights)\n",
    "    # # new_weights /= np.sum(new_weights) # do this after mean and covariance update\n",
    "\n",
    "    # # update means\n",
    "    # new_means = []\n",
    "    # for i in range(n_components):\n",
    "    #     ssum = 0\n",
    "    #     psum = 0\n",
    "    #     for j in range(len(compressed_measurements)):\n",
    "    #         den = 0\n",
    "    #         for k in range(n_components):\n",
    "    #             den += estimated_gmm.weights_[k] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[k], cov=y_gmms[sample_measurement_matrix[j]].covariances_[k])\n",
    "    #         p_ik = estimated_gmm.weights_[i] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[i], cov=y_gmms[sample_measurement_matrix[j]].covariances_[i]) / den\n",
    "    #         C_ik = np.linalg.pinv(measurement_matrices[sample_measurement_matrix[j]].T @ noise_inverse @ measurement_matrices[sample_measurement_matrix[j]] + estimated_gmm.precisions_[i])\n",
    "    #         n_ik = estimated_gmm.means_[i] + (C_ik @ measurement_matrices[sample_measurement_matrix[j]].T @ noise_inverse @ (compressed_measurements[j] - measurement_matrices[sample_measurement_matrix[j]] @ estimated_gmm.means_[i]))\n",
    "    #         ssum += p_ik * n_ik\n",
    "    #         psum += p_ik\n",
    "    #     new_means.append(ssum / psum)\n",
    "\n",
    "    # # update covariances\n",
    "    # new_covariances = []\n",
    "    # for i in range(n_components):\n",
    "    #     ssum = 0\n",
    "    #     psum = 0\n",
    "    #     for j in range(len(compressed_measurements)):\n",
    "    #         den = 0\n",
    "    #         for k in range(n_components):\n",
    "    #             den += y_gmms[sample_measurement_matrix[j]].weights_[k] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[k], cov=y_gmms[sample_measurement_matrix[j]].covariances_[k])\n",
    "    #         p_ik = y_gmms[sample_measurement_matrix[j]].weights_[i] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[i], cov=y_gmms[sample_measurement_matrix[j]].covariances_[i]) / den\n",
    "    #         C_ik = np.linalg.pinv((measurement_matrices[sample_measurement_matrix[j]].T @ noise_inverse @ measurement_matrices[sample_measurement_matrix[j]]) + estimated_gmm.precisions_[i])\n",
    "    #         n_ik = estimated_gmm.means_[i] + C_ik @ (measurement_matrices[sample_measurement_matrix[j]].T @ noise_inverse @ (compressed_measurements[j] - measurement_matrices[sample_measurement_matrix[j]] @ estimated_gmm.means_[i]))\n",
    "    #         ssum += p_ik * ((n_ik - new_means[i]) @ (n_ik - new_means[i]).T + C_ik)\n",
    "    #         psum += p_ik\n",
    "    #     new_covariances.append(ssum / psum)\n",
    "\n",
    "\n",
    "    estimated_gmm.weights_ = get_new_weights(compressed_measurements, y_gmms, sample_measurement_matrix)\n",
    "    estimated_gmm.means_ = get_new_means(compressed_measurements, y_gmms, sample_measurement_matrix, estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "    estimated_gmm.covariances_ = get_new_covariances(compressed_measurements, y_gmms, sample_measurement_matrix, estimated_gmm, measurement_matrices, noise_covariance_matrix, estimated_gmm.means_)\n",
    "    precision_array = [np.linalg.pinv(cov) for cov in estimated_gmm.covariances_]\n",
    "    estimated_gmm.precisions_ = np.array(precision_array)\n",
    "    estimated_gmm.precisions_cholesky_ = np.array([np.linalg.cholesky(prec) for prec in estimated_gmm.precisions_])\n",
    "\n",
    "    y_gmms = form_y_gmms(estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "    log_likelihoods.append(get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, y_gmms))\n",
    "    print(f\"Iteration {iter_counter + 1} log-likelihood: {log_likelihoods[-1]}\")\n",
    "    iter_counter += 1\n",
    "    if abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol or log_likelihoods[-1] > ideal_log_likelihood:\n",
    "        if abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol:\n",
    "            print(\"Converged\")\n",
    "        else:\n",
    "            print(\"Ideal log-likelihood reached, stopping to avoid overfitting\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 16.51381263702895\n"
     ]
    }
   ],
   "source": [
    "# get test sample set, take compressive measurements, reconstruct and check rmse\n",
    "n_test = 500\n",
    "test_sample_set = my_test_gmm.sample(n_samples=n_test)[0]\n",
    "test_compressed_measurements = []\n",
    "test_sample_matrix_indices = []\n",
    "for i in range(n_test):\n",
    "    matrix_index = np.random.randint(0, len(measurement_matrices))\n",
    "    test_sample_matrix_indices.append(matrix_index)\n",
    "    test_compressed_measurements.append(measurement_matrices[matrix_index] @ test_sample_set[i])\n",
    "\n",
    "test_reconstructed_samples = []\n",
    "y_gmms = form_y_gmms(estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "for i in range(n_test):\n",
    "    y_gmm = y_gmms[test_sample_matrix_indices[i]]\n",
    "    reconstructed_sample = np.zeros(estimated_gmm.means_[0].shape)\n",
    "    psum = 0\n",
    "    for j in range(n_components):\n",
    "        p_ik = y_gmm.weights_[j] * multivariate_normal.pdf(test_compressed_measurements[i], mean=y_gmm.means_[j], cov=y_gmm.covariances_[j]) / np.exp(y_gmm.score(test_compressed_measurements[i].reshape(1, -1)))\n",
    "        psum += p_ik\n",
    "        n_ik = get_x_reconstruction(measurement_matrices[test_sample_matrix_indices[i]], noise_covariance_matrix, estimated_gmm.covariances_[j], estimated_gmm.means_[j], test_compressed_measurements[i])\n",
    "        reconstructed_sample += p_ik * n_ik\n",
    "    assert abs(psum - 1) < 1e-3\n",
    "    test_reconstructed_samples.append(reconstructed_sample)\n",
    "\n",
    "test_rmse = np.sqrt(np.mean(np.sum((test_sample_set - test_reconstructed_samples) ** 2, axis=1)))\n",
    "print(f\"Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.28869673  4.28943261  0.43035924  3.58168006 -0.90695049  2.99777537\n",
      " -0.18597649  1.18841429  1.79079885  0.80694399  3.48087534  2.99379931\n",
      "  0.08533483  1.60111894  3.20262747  2.78492888  4.35821626  2.32312332\n",
      "  2.36550731 -0.46538501]\n",
      "[ 2.97553047  1.90800387 -0.08012204  1.11290374  0.18804086  2.33659121\n",
      "  0.83482777 -0.15348891  0.26239241  1.19251821  1.20329352  1.84571248\n",
      " -0.5308645   0.64560709  1.10076739  3.29614228  4.05775476  0.22499558\n",
      " -0.36650198  1.58351274]\n"
     ]
    }
   ],
   "source": [
    "print(test_sample_set[2])\n",
    "print(test_reconstructed_samples[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
