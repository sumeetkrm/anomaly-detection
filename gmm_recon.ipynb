{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmm recovery from a synthetic gmm\n",
    "# make a representative gmm\n",
    "def make_gmm(n_components, n_features, random_state=0):\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "    gmm.means_ = np.random.rand(n_components, n_features)\n",
    "    covariance_array = []\n",
    "    for i in range(n_components):\n",
    "        rand_matrix = np.random.rand(n_features, n_features)\n",
    "        covariance_array.append(np.dot(rand_matrix, rand_matrix.T))\n",
    "\n",
    "    precision_array = [np.linalg.pinv(covariance_array[i]) for i in range(n_components)]\n",
    "    gmm.precisions_ = np.array(precision_array)\n",
    "    gmm.precisions_cholesky_ = np.array([np.linalg.cholesky(precision_array[i]) for i in range(n_components)])\n",
    "    gmm.covariances_ = np.array(covariance_array)\n",
    "    \n",
    "    gmm.weights_ = np.random.rand(n_components)\n",
    "    gmm.weights_ /= np.sum(gmm.weights_)\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-5.44149323, -4.04280332, -3.89185993, -3.79538471, -2.332354  ,\n",
       "         -3.56951212, -4.89904712, -4.49417209, -2.63450021, -3.21219345,\n",
       "         -2.07171713, -4.50845398, -1.29492845, -2.97777404, -5.18532151,\n",
       "         -5.57904983, -4.9643353 , -5.40788313, -4.37305521, -2.66455146]]),\n",
       " array([4]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 5\n",
    "n_features = 20\n",
    "\n",
    "my_test_gmm = make_gmm(n_components, n_features)\n",
    "my_test_gmm.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take compressive meaurements\n",
    "# make 10 random measurement matrices\n",
    "n_measurements = ((n_features * 4) // 10) # 40% of the features\n",
    "\n",
    "measurement_matrices = []\n",
    "for i in range(10):\n",
    "    measurement_matrices.append(np.random.rand(n_measurements, n_features))\n",
    "\n",
    "# generate samples from the gmm\n",
    "n_samples = 1000\n",
    "samples = my_test_gmm.sample(n_samples)\n",
    "\n",
    "# take measurements\n",
    "compressed_measurements = []\n",
    "sample_measurement_matrix = []\n",
    "for i in range(n_samples):\n",
    "    matrix_index = np.random.randint(0, len(measurement_matrices))\n",
    "    sample_measurement_matrix.append(matrix_index)\n",
    "    compressed_measurements.append(np.dot(measurement_matrices[matrix_index], samples[0][i]))\n",
    "\n",
    "\n",
    "noise_std_dev = 0.1\n",
    "noise_covariance_matrix = np.eye(n_measurements) * noise_std_dev * noise_std_dev\n",
    "compressed_measurements = np.array(compressed_measurements)\n",
    "compressed_measurements += np.random.normal(0, noise_std_dev, compressed_measurements.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of the 10 measurement matrices we have a separate gmm in the y-domain so let us make all of them\n",
    "\n",
    "def form_y_gmms(x_gmm, measurement_matrices, noise_covariance_matrix):\n",
    "    gmm_list = []\n",
    "    noise_inverse = np.linalg.inv(noise_covariance_matrix)\n",
    "    for i in range(len(measurement_matrices)):\n",
    "        gmm_sample = GaussianMixture(n_components=n_components, random_state=0)\n",
    "        gmm_sample.means_ = x_gmm.means_ @ measurement_matrices[i].T\n",
    "        covariance_array = []\n",
    "        for j in range(n_components):\n",
    "            covariance_array.append(np.linalg.pinv(noise_inverse - ((noise_inverse @ measurement_matrices[i]) @ np.linalg.pinv((measurement_matrices[i].T @ noise_inverse @ measurement_matrices[i]) + np.linalg.pinv(x_gmm.covariances_[j])) @ measurement_matrices[i].T @ noise_inverse)))\n",
    "\n",
    "        gmm_sample.covariances_ = np.array(covariance_array)\n",
    "        gmm_sample.weights_ = x_gmm.weights_\n",
    "\n",
    "        precision_array = [np.linalg.pinv(gmm_sample.covariances_[i]) for i in range(n_components)]\n",
    "        gmm_sample.precisions_ = np.array(precision_array)\n",
    "        gmm_sample.precisions_cholesky_ = np.array([np.linalg.cholesky(precision_array[i]) for i in range(n_components)])\n",
    "        \n",
    "        gmm_list.append(gmm_sample)\n",
    "    return gmm_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, gmm_list):\n",
    "    log_likelihood = 0\n",
    "    for i in range(len(compressed_measurements)):\n",
    "        # print(gmm_list[sample_measurement_matrix[i]].score(compressed_measurements[i].reshape(1, -1)))\n",
    "        log_likelihood += gmm_list[sample_measurement_matrix[i]].score(compressed_measurements[i].reshape(1, -1))\n",
    "\n",
    "    return log_likelihood/len(compressed_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial log likelihood:  -18.57757356820687\n",
      "Ideal log likelihood:  -17.644096683594977\n",
      "-22.391154379786933\n",
      "-28.899916278571755\n",
      "-39.737552662789085\n",
      "-55.72151231756294\n",
      "-77.15513159736045\n",
      "-103.64957595138159\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sumeet/btp/anomaly-detection/gmm_recon.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sumeet/btp/anomaly-detection/gmm_recon.ipynb#W4sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         den \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m y_gmms[sample_measurement_matrix[j]]\u001b[39m.\u001b[39mweights_[k] \u001b[39m*\u001b[39m multivariate_normal\u001b[39m.\u001b[39mpdf(compressed_measurements[j], mean\u001b[39m=\u001b[39my_gmms[sample_measurement_matrix[j]]\u001b[39m.\u001b[39mmeans_[k], cov\u001b[39m=\u001b[39my_gmms[sample_measurement_matrix[j]]\u001b[39m.\u001b[39mcovariances_[k])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sumeet/btp/anomaly-detection/gmm_recon.ipynb#W4sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     p_ik \u001b[39m=\u001b[39m y_gmms[sample_measurement_matrix[j]]\u001b[39m.\u001b[39mweights_[i] \u001b[39m*\u001b[39m multivariate_normal\u001b[39m.\u001b[39mpdf(compressed_measurements[j], mean\u001b[39m=\u001b[39my_gmms[sample_measurement_matrix[j]]\u001b[39m.\u001b[39mmeans_[i], cov\u001b[39m=\u001b[39my_gmms[sample_measurement_matrix[j]]\u001b[39m.\u001b[39mcovariances_[i]) \u001b[39m/\u001b[39m den\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sumeet/btp/anomaly-detection/gmm_recon.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     n_ik \u001b[39m=\u001b[39m (estimated_gmm\u001b[39m.\u001b[39mmeans_[i] \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mpinv((measurement_matrices[sample_measurement_matrix[j]]\u001b[39m.\u001b[39;49mT \u001b[39m@\u001b[39;49m noise_inverse \u001b[39m@\u001b[39;49m measurement_matrices[sample_measurement_matrix[j]]) \u001b[39m+\u001b[39;49m estimated_gmm\u001b[39m.\u001b[39;49mprecisions_[i]) \u001b[39m@\u001b[39m (measurement_matrices[sample_measurement_matrix[j]]\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m noise_inverse \u001b[39m@\u001b[39m (compressed_measurements[j] \u001b[39m-\u001b[39m y_gmms[sample_measurement_matrix[j]]\u001b[39m.\u001b[39mmeans_[i])))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sumeet/btp/anomaly-detection/gmm_recon.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     ssum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m p_ik \u001b[39m*\u001b[39m n_ik\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sumeet/btp/anomaly-detection/gmm_recon.ipynb#W4sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m new_means\u001b[39m.\u001b[39mappend(ssum \u001b[39m/\u001b[39m new_weights[i])\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mpinv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py:1998\u001b[0m, in \u001b[0;36mpinv\u001b[0;34m(a, rcond, hermitian)\u001b[0m\n\u001b[1;32m   1996\u001b[0m     \u001b[39mreturn\u001b[39;00m wrap(res)\n\u001b[1;32m   1997\u001b[0m a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mconjugate()\n\u001b[0;32m-> 1998\u001b[0m u, s, vt \u001b[39m=\u001b[39m svd(a, full_matrices\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, hermitian\u001b[39m=\u001b[39;49mhermitian)\n\u001b[1;32m   2000\u001b[0m \u001b[39m# discard small singular values\u001b[39;00m\n\u001b[1;32m   2001\u001b[0m cutoff \u001b[39m=\u001b[39m rcond[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, newaxis] \u001b[39m*\u001b[39m amax(s, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py:1657\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         gufunc \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39msvd_n_s\n\u001b[1;32m   1656\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->DdD\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->ddd\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1657\u001b[0m u, s, vh \u001b[39m=\u001b[39m gufunc(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[1;32m   1658\u001b[0m u \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1659\u001b[0m s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mastype(_realType(result_t), copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialise estimate of the original gmm\n",
    "# assumption is that we know the number of components required to model the original gmm\n",
    "estimated_gmm = make_gmm(n_components, n_features, random_state=10)\n",
    "\n",
    "# perform updates on the estimated gmm\n",
    "# log likelihoods stores the log likelihoods of the estimated gmm computed on the samples drawn from the original gmm, our objective is to maximise this\n",
    "tol = 1e-3\n",
    "log_likelihoods = []\n",
    "log_likelihoods.append(get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, form_y_gmms(estimated_gmm, measurement_matrices, noise_covariance_matrix)))\n",
    "\n",
    "print(\"Initial log likelihood: \", log_likelihoods[-1])\n",
    "print(\"Ideal log likelihood: \", get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, form_y_gmms(my_test_gmm, measurement_matrices, noise_covariance_matrix)))\n",
    "\n",
    "noise_inverse =  np.linalg.inv(noise_covariance_matrix)\n",
    "y_gmms = form_y_gmms(estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "\n",
    "while True:\n",
    "    # perform updates\n",
    "    # for each of the 10 measurement matrices we have a separate gmm in the y-domain so let us make all of them\n",
    "    # update weights\n",
    "    new_weights = []\n",
    "    for i in range(n_components):\n",
    "        ssum = 0\n",
    "        for j in range(len(compressed_measurements)):\n",
    "            den = 0\n",
    "            for k in range(n_components):\n",
    "                den += y_gmms[sample_measurement_matrix[j]].weights_[k] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[k], cov=y_gmms[sample_measurement_matrix[j]].covariances_[k])\n",
    "            ssum += y_gmms[sample_measurement_matrix[j]].weights_[i] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[i], cov=y_gmms[sample_measurement_matrix[j]].covariances_[i]) / den\n",
    "        new_weights.append(ssum)\n",
    "    new_weights = np.array(new_weights)\n",
    "    # new_weights /= np.sum(new_weights) # do this after mean and covariance update\n",
    "\n",
    "    # update means\n",
    "    new_means = []\n",
    "    for i in range(n_components):\n",
    "        ssum = 0\n",
    "        for j in range(len(compressed_measurements)):\n",
    "            den = 0\n",
    "            for k in range(n_components):\n",
    "                den += y_gmms[sample_measurement_matrix[j]].weights_[k] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[k], cov=y_gmms[sample_measurement_matrix[j]].covariances_[k])\n",
    "            p_ik = y_gmms[sample_measurement_matrix[j]].weights_[i] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[i], cov=y_gmms[sample_measurement_matrix[j]].covariances_[i]) / den\n",
    "            n_ik = (estimated_gmm.means_[i] + np.linalg.pinv((measurement_matrices[sample_measurement_matrix[j]].T @ noise_inverse @ measurement_matrices[sample_measurement_matrix[j]]) + estimated_gmm.precisions_[i]) @ (measurement_matrices[sample_measurement_matrix[j]].T @ noise_inverse @ (compressed_measurements[j] - y_gmms[sample_measurement_matrix[j]].means_[i])))\n",
    "            ssum += p_ik * n_ik\n",
    "        new_means.append(ssum / new_weights[i])\n",
    "\n",
    "    # update covariances\n",
    "    new_covariances = []\n",
    "    for i in range(n_components):\n",
    "        ssum = 0\n",
    "        for j in range(len(compressed_measurements)):\n",
    "            den = 0\n",
    "            for k in range(n_components):\n",
    "                den += y_gmms[sample_measurement_matrix[j]].weights_[k] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[k], cov=y_gmms[sample_measurement_matrix[j]].covariances_[k])\n",
    "            p_ik = y_gmms[sample_measurement_matrix[j]].weights_[i] * multivariate_normal.pdf(compressed_measurements[j], mean=y_gmms[sample_measurement_matrix[j]].means_[i], cov=y_gmms[sample_measurement_matrix[j]].covariances_[i]) / den\n",
    "            C_ik = np.linalg.pinv((measurement_matrices[sample_measurement_matrix[j]].T @ noise_inverse @ measurement_matrices[sample_measurement_matrix[j]]) + estimated_gmm.precisions_[i])\n",
    "            n_ik = estimated_gmm.means_[i] + C_ik @ (measurement_matrices[sample_measurement_matrix[j]].T @ noise_inverse @ (compressed_measurements[j] - y_gmms[sample_measurement_matrix[j]].means_[i]))\n",
    "            ssum += p_ik * ((n_ik - new_means[i]) @ (n_ik - new_means[i]).T + C_ik)\n",
    "        new_covariances.append(ssum / new_weights[i])\n",
    "\n",
    "    estimated_gmm.weights_ = new_weights / np.sum(new_weights)\n",
    "    estimated_gmm.means_ = np.array(new_means)\n",
    "    estimated_gmm.covariances_ = np.array(new_covariances)\n",
    "    estimated_gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(estimated_gmm.covariances_))\n",
    "    estimated_gmm.precisions_ = np.linalg.inv(estimated_gmm.covariances_)\n",
    "\n",
    "    y_gmms = form_y_gmms(estimated_gmm, measurement_matrices, noise_covariance_matrix)\n",
    "    log_likelihoods.append(get_log_likelihood(measurement_matrices, compressed_measurements, sample_measurement_matrix, y_gmms))\n",
    "    print(log_likelihoods[-1])\n",
    "    if abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol:\n",
    "        break\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
