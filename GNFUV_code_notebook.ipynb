{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39131f8",
   "metadata": {},
   "source": [
    "# Compressed sensing using GMMs on GNFUV dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf8dd1",
   "metadata": {},
   "source": [
    "Dataset found at: https://archive.ics.uci.edu/ml/datasets/GNFUV+Unmanned+Surface+Vehicles+Sensor+Data+Set+2#\n",
    "The data-set comprises (4) sets of mobile sensor readings data (humidity, temperature) corresponding to a swarm of four (4) Unmanned Surface Vehicles (USVs).\n",
    "The swarm of the USVs is moving according to a GPS pre-defined trajectory, whose relative way-points are in the figure. The USVs are floating over the sea surface in a coastal area of Athens (Greece).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e198fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f70a8",
   "metadata": {},
   "source": [
    "Reused code from Nimay's BTP code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4df2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_l2(v, mat):\n",
    "    if np.linalg.matrix_rank(mat) < mat.shape[0]:\n",
    "        m = v.T @ np.linalg.pinv(mat) @ v\n",
    "    else:\n",
    "        # print(\"hi \", np.linalg.det(mat), np.linalg.matrix_rank(mat))\n",
    "        m = v.T @ np.linalg.inv(mat) @ v\n",
    "    return m\n",
    "\n",
    "def decode(model, A, y, sigma = 1e-7):\n",
    "    x_hat = np.empty(model.means_.shape)\n",
    "    cost = []\n",
    "    var_noise = sigma * np.eye(A.shape[0])\n",
    "\n",
    "    for j in range(model.means_.shape[0]):\n",
    "        var_j, mu_j = model.covariances_[j], model.means_[j]\n",
    "        x_hat_j = var_j @ A.T @ np.linalg.inv(A @ var_j @ A.T + var_noise) @ (y - A @ mu_j) + mu_j\n",
    "        # print(np.linalg.det(var_j))\n",
    "        # print(var_j)\n",
    "        try:\n",
    "            cost_j = weighted_l2(y - A @ x_hat_j, var_noise) + weighted_l2(x_hat_j - mu_j, var_j) + np.log(np.linalg.det(var_j),where=np.linalg.det(var_j)>0)\n",
    "        except:\n",
    "            print(\"y - A @ x_hat_j\",y - A @ x_hat_j , \"\\n\", var_noise,\"\\n\",y,A)\n",
    "        # print(np.linalg.det(var_j))\n",
    "        x_hat[j] = x_hat_j\n",
    "        cost.append(cost_j)\n",
    "\n",
    "    j = np.argmin(cost)\n",
    "    # print(j)\n",
    "    return x_hat[j]\n",
    "\n",
    "def PSNR(x_, x_t):\n",
    "    assert x_.shape == x_t.shape\n",
    "    peak = np.max(x_) - np.min(x_)\n",
    "    mse = np.mean(((x_ - x_t)/peak)**2, axis=1)\n",
    "    print(type(mse),mse)\n",
    "    psnr = -10*np.log10(mse)\n",
    "    return np.mean(psnr), np.min(psnr), np.max(psnr), np.std(psnr)\n",
    "\n",
    "def plot_data(xs, ys, path = 'PSNR.png', ylabel='Avg PSNR'):\n",
    "    plt.plot(xs, ys, marker=\"o\")\n",
    "    plt.xlabel('# of measurements')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    for x,y in zip(xs,ys):\n",
    "        label = f\"({x},{round(y,2)})\"\n",
    "        plt.annotate(label, (x,y), textcoords=\"offset points\", xytext=(-5,10), ha='center')\n",
    "#     plt.tight_layout()\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmse(model, A, sigma = 1e-5):\n",
    "    cost = 0\n",
    "    var_noise = sigma * np.eye(A.shape[0])\n",
    "    for j in range(model.means_.shape[0]):\n",
    "        var_j = model.covariances_[j]\n",
    "        m_j = np.trace(var_j - var_j @ A.T @ np.linalg.inv(A @ var_j @ A.T + var_noise) @ A @ var_j)\n",
    "        cost += model.weights_[j] * m_j\n",
    "    return cost\n",
    "\n",
    "def optimize_mat(model, A_):\n",
    "    A = A_.copy()\n",
    "    costs = []\n",
    "    for it in tqdm(range(20)):\n",
    "        cnt = 0\n",
    "        for i in range(A.shape[0]):\n",
    "            for j in range(A.shape[1]):\n",
    "                # flip the bit only if mmse is reduced\n",
    "                cost = mmse(model, A)\n",
    "                A[i, j] = 1 - A[i, j]\n",
    "                cnt += 1\n",
    "                if cost <= mmse(model, A):\n",
    "                    A[i, j] = 1 - A[i, j]\n",
    "                    cnt -= 1\n",
    "        # break the loop if no improvement\n",
    "        if cnt == 0:\n",
    "            break\n",
    "        costs.append(mmse(model, A))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c724c1",
   "metadata": {},
   "source": [
    "The time-series data of GNFUV dataset is arranged in arrays of patches, for patch_size = 40, we will take 20 time-instances and make an array \\[ temperature_1, humidity_1, ... , temperature_20, humidity_20 \\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db724935",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Patch size\n",
    "patch_size = 20*2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37a474",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Dataset has some empty values, which we remove. We also sort the dataset by the time, since we would transmit temporally close data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87089dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = []\n",
    "\n",
    "for experiment_number  in [1,2]:\n",
    "    \n",
    "    for USV_number in [2,3,4,5]:\n",
    "        path = \"CNFUV_Datasets/Datasets/Data_Experiment_%d/pi%d.xlsx\"% (experiment_number, USV_number)\n",
    "        dataframe = pandas.read_excel(path)\n",
    "\n",
    "        dataframe = dataframe.mask(dataframe.eq(\" None\")).dropna()\n",
    "        dataframe = dataframe.mask(dataframe.eq(\"None\")).dropna()\n",
    "        dataframe = dataframe.sort_values(by=['time'])\n",
    "        dataframe = dataframe[['Humidity','Temperature']]\n",
    "        \n",
    "        dataframe = dataframe[:(dataframe.shape[0]//patch_size)*patch_size]\n",
    "        data = dataframe.to_numpy().flatten()\n",
    "        data = data.reshape(-1,patch_size)\n",
    "        print(data.shape)\n",
    "        full_data.extend(data)\n",
    "full_data = np.array(full_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ccd05d",
   "metadata": {},
   "source": [
    "We divide the dataset into 90:10 train:test ratio and set other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b1795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = full_data.shape[0]//10 *9\n",
    "print(n_train,full_data.shape[0])\n",
    "n_test = full_data.shape[0] - n_train\n",
    "n_component_overall = 5\n",
    "cnt = 5\n",
    "n_init = 5\n",
    "\n",
    "to_train = 1\n",
    "use_mat = 0\n",
    "\n",
    "np.random.shuffle(data)\n",
    "train_data = full_data[:n_train]\n",
    "test_data = full_data[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76925288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"full_data.shape: \",full_data.shape)\n",
    "print(\"len(train_data): \",len(train_data))\n",
    "print(\"len(test_data): \",len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42254236",
   "metadata": {},
   "source": [
    "Now, we consider the unoptimised case and the optimised case. In the optimised case, we iterate over a range of possible n_components to get the best fit using the BIC criteria. We also use the sensing matrix optimisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebba14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A  = np.random.binomial(1, 0.5, size=(patch_size//5,patch_size))\n",
    "\n",
    "unoptimised = 0\n",
    "optimised = 0\n",
    "val_signal_mean = 0\n",
    "runs = 20\n",
    "for run in range(runs):\n",
    "    folder_name = 'GNFUV_results/latest/'\n",
    "    if to_train:\n",
    "        model = GaussianMixture(n_components=n_component_overall, n_init=n_init,  max_iter=200, init_params='random')\n",
    "        model.fit(train_data)\n",
    "\n",
    "        lowest_bic = np.infty\n",
    "        bic = []\n",
    "        n_components_range = range(1, 10)\n",
    "\n",
    "        for n_component in n_components_range:\n",
    "            # Fit a Gaussian mixture with EM\n",
    "            gmm = GaussianMixture(\n",
    "                n_components=n_component, n_init=n_init,  max_iter=200,\n",
    "            )\n",
    "            gmm.fit(train_data)\n",
    "            bic.append(gmm.bic(train_data))\n",
    "            if bic[-1] < lowest_bic:\n",
    "                lowest_bic = bic[-1]\n",
    "                best_gmm = gmm\n",
    "                n_component_overall = n_component\n",
    "        model_opt = best_gmm\n",
    "        print(\"n_component_overall: \",n_component_overall)\n",
    "        patch_err = []\n",
    "        patch_err_opt = []\n",
    "        A_opt = optimize_mat(model, A)\n",
    "        for j in range(len(test_data)):\n",
    "            x = test_data[j]\n",
    "            x_hat = np.zeros(x.shape)\n",
    "            y = A @ x\n",
    "            y_opt = A_opt @ x\n",
    "            x_hat = decode(model, A, y)\n",
    "            x_hat_opt = decode(model_opt, A_opt, y_opt)\n",
    "            # Mean Absolute Percentage Error (MAPE)\n",
    "            patch_err.append(np.mean(np.abs( np.divide(x - np.round(x_hat),x) )))\n",
    "            patch_err_opt.append(np.mean(np.abs( np.divide(x - np.round(x_hat_opt),x) )))\n",
    "        #     if patch_err[-1] > 1:\n",
    "        #         print(\"x:\\n\",x,\"x_hat:\\n\",x_hat,\"y:\\n\",y)\n",
    "        val_err = np.mean(patch_err)\n",
    "        unoptimised +=val_err\n",
    "        val_err_opt = np.mean(patch_err_opt)\n",
    "        optimised += val_err_opt\n",
    "        val_signal_mean += np.mean(test_data)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca5699",
   "metadata": {},
   "source": [
    "Print the percentage errors for the unoptimised and optimised case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dbe05c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_err = unoptimised/runs\n",
    "val_err_opt = optimised/runs\n",
    "val_signal_mean /= runs\n",
    "print(val_err,val_err_opt,val_signal_mean)\n",
    "print(\"Percentage error: \" ,val_err*100, val_err_opt*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f820c",
   "metadata": {},
   "source": [
    "Play a sound after a long simulation ends to alert the user that it is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "duration = 3000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, label='Original', color='C0')\n",
    "plt.plot(x_hat_opt, label='Reconstructed', color='C1')\n",
    "print(y,x,\"\\nx_hat\",x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b2be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e318814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c590a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d31f477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19ee3195",
   "metadata": {},
   "source": [
    "Code dump ( partially modified code, older version code, users other than the coder can ignore) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4aef4cae",
   "metadata": {},
   "source": [
    "folder_name = 'GNFUV_results/latest/'\n",
    "if to_train:\n",
    "#     model = GaussianMixture(n_components=n_components, n_init=n_init, verbose=1, max_iter=200, init_params='random')\n",
    "#     model.fit(train_data)\n",
    "\n",
    "    lowest_bic = np.infty\n",
    "    bic = []\n",
    "    n_components_range = range(1, 10)\n",
    "\n",
    "    for n_components in n_components_range:\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm = GaussianMixture(\n",
    "            n_components=n_components, n_init=n_init,  max_iter=200,\n",
    "        )\n",
    "        gmm.fit(train_data)\n",
    "        bic.append(gmm.bic(train_data))\n",
    "        if bic[-1] < lowest_bic:\n",
    "            lowest_bic = bic[-1]\n",
    "            best_gmm = gmm\n",
    "            n_component_overall = n_components\n",
    "    model_opt = best_gmm\n",
    "    print(\"n_component_overall: \",n_component_overall)\n",
    "    if os.path.isdir(folder_name):\n",
    "        for f in os.listdir(folder_name):\n",
    "            if not os.path.isdir(folder_name + f):\n",
    "                os.remove(folder_name + f)\n",
    "    else:\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Save the model as a pickle in a file\n",
    "    joblib.dump(model, folder_name + 'model.pkl')\n",
    "else:\n",
    "    \n",
    "    # Load the model from the file\n",
    "    model = joblib.load(folder_name + 'model.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d023add7",
   "metadata": {},
   "source": [
    "A  = np.random.binomial(1, 0.5, size=(patch_size//5,patch_size))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7c546ed",
   "metadata": {},
   "source": [
    "patch_err = []\n",
    "patch_err_opt = []\n",
    "\n",
    "for j in range(len(test_data)):\n",
    "    x = test_data[j]\n",
    "    x_hat = np.zeros(x.shape)\n",
    "    y = A @ x\n",
    "    x_hat = decode(model, A, y)\n",
    "    x_hat_opt = decode(model_opt, A, y)\n",
    "    patch_err.append(np.mean(np.square(x - np.round(x_hat))))\n",
    "    patch_err_opt.append(np.mean(np.square(x - np.round(x_hat_opt))))\n",
    "#     if patch_err[-1] > 1:\n",
    "#         print(\"x:\\n\",x,\"x_hat:\\n\",x_hat,\"y:\\n\",y)\n",
    "val_err = np.mean(patch_err)\n",
    "val_err_opt = np.mean(patch_err_opt)\n",
    "val_signal_mean = np.mean(test_data)\n",
    "print(val_err,val_err_opt,val_signal_mean)\n",
    "print(\"Percentage error: \" ,val_err/val_signal_mean*100, val_err_opt/val_signal_mean*100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbc33e8d",
   "metadata": {},
   "source": [
    "# for var in model.covariances_:\n",
    "#     print(var)\n",
    "#     print(np.linalg.det(var), np.linalg.matrix_rank(var))\n",
    "#     # print(np.linalg.det(np.linalg.inv(var)))\n",
    "# for var in model.weights_:\n",
    "#     print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
